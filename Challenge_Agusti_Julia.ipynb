{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use PANDAS library for this project\n",
    "import pandas as pd\n",
    "# We will use MatPlot library for the visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will import to a DataFrame only the necessary fields for this project from the dataset: 'hour' and 'click'\n",
    "fields= ['hour','click']\n",
    "df=pd.read_csv('./data/train.gz',compression='gzip',skipinitialspace=True,usecols=fields)\n",
    "# We have a look to the data structure\n",
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We format the 'hour' field into datetime data type under a new field called 'date'. This will help to process the time series\n",
    "df['date'] = pd.to_datetime(df['hour'].astype(str),format='%y%m%d%H')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We redefine as index for the DataFrame the new field 'date' ( this simplifies the processing and the visualizations )\n",
    "df = df.set_index(pd.DatetimeIndex(df['date']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PART 1: CTR over TIME ########\n",
    "######################################\n",
    "# Now we are ready to aggregate per hour the timeseries into a new DataFrame\n",
    "# While we will not need the hour field anymore in the aggregated DataFrame we reuse it to make the 'hour' count \n",
    "# and record it in this field. We can save some memory with this method.\n",
    "df_agg=df.resample('h').agg({\"hour\":'size',\"click\":'sum'})\n",
    "\n",
    "\n",
    "# We rename  the field hour to 'impressions': it contains the total count of \n",
    "# advertisement visualizations (how often they are shown)\n",
    "df_agg.rename({'hour':'impressions'},axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We have a look to the data structure\n",
    "df_agg.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We list some of the records to check that everything looks fine\n",
    "df_agg.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the CTR ( Click Through-Rates) and we store it in a new column on the DataFrame\n",
    "df_agg['ctr'] = df_agg.apply(lambda row: row['click'] / row['impressions'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let´s check how it looks like\n",
    "df_agg.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to visualize the the resulting time series\n",
    "# We use seaborn style defaults and set the default figure size to fit the screen\n",
    "sns.set(rc={'figure.figsize':(16, 4)})\n",
    "ax=df_agg.plot(y='ctr')\n",
    "ax.set_ylabel('CTR')\n",
    "ax.set_xlabel('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let´s see how the Histogram looks like ( beside in the second part of the challenge we assume that \n",
    "# it is a Gaussian Distribution )\n",
    "df_agg.hist(column='ctr')\n",
    "plt.title(\"Histogram of CTR\")\n",
    "plt.xlabel(\"CTR\")\n",
    "plt.ylabel(\"Frequency (count)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PART 2: Outlier Detection ########\n",
    "##########################################\n",
    "\n",
    "# First we calculate the standard deviation for the CTR\n",
    "ctr_std=df_agg['ctr'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that the Moving Average to compute is the Simple Moving Average (SMA). It would be possible to use a \n",
    "# Exponential Moving Average ( EMA ) but it is out of this exercise scope to assess the benefits\n",
    "\n",
    "# There is no clue for the Window Size to use to calculate the Moving Average. We can think that the SMA is a Low Pass Filter\n",
    "# for a signal that discriminates the noise on this signal. We can see the Window Size (in the frequency domain for the signal)\n",
    "# as the cut frequency of this Low Pass Filter. \n",
    "\n",
    "# One possible method to decide the Window Size for the SMA is to consider the compromise between the 'smoothness' \n",
    "# ( noise reduction ) and the 'delay' on the smoothed signal that SMA introduces depending on the Window Size. If we compute the \n",
    "# SAD ( sum of absolute differences between the SMA obtained signal and the original ) for different window sizes we \n",
    "# could find a Window Selection Criteria\n",
    "\n",
    "# We create a DataFrame for the SAD values for different Window Sizes\n",
    "sad=pd.DataFrame()\n",
    "\n",
    "# We calculate the SMA for a Window size between 2 and 12 samples and we keep them as a new column on the agreggated\n",
    "# DataFrame from the previous exercices\n",
    "\n",
    "for i in range(2,12):\n",
    "    # We calculate the SMAs for a Window size between 2 and 12 samples and we keep them as a new column on the aggregated\n",
    "    # aggregated DataFrame from the previous exercice. This means that we will end up with 10 SMA TimeSeries.\n",
    "    df_agg['MA{}'.format(i)]=df_agg.rolling(window=i)['ctr'].mean()\n",
    "    # We calculate the SAD for the corresponding Window as the difference between the SMA and the original signal\n",
    "    sad.at[format(i),'SAD']=abs(df_agg['MA{}'.format(i)]-df_agg['ctr']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let´s have a look how the new columns with the SMAs look like\n",
    "df_agg.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let´s check the SDA values\n",
    "sad.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let´s plot the SAD. As we will see there is a point where increasing the Window Size doesn´t produce much more benefit\n",
    "# in the smoothing and it takes longer to compute and it lags the original signal more.\n",
    "# We will pick the smallest window size where the SAD starts to flatten out. In our case the proper window size is 8 samples, \n",
    "# as we can see in the graph bellow.\n",
    "ax=sad.plot(y='SAD',use_index=True)\n",
    "plt.ylabel('SAD')\n",
    "plt.xlabel('Window Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now we are ready to detect the outliers under the criteria 1.5 standard deviations apart from its calculated moving\n",
    "# average ( in our case MA8 )\n",
    "criteria=1.5*ctr_std\n",
    "\n",
    "# To find the Outliers we use the method 'WHERE' over the aggregated DataFrame, using the specified criteria.\n",
    "# We assign the value of the outliers on a new column called 'outliers' in the aggregated DataFrame linked to \n",
    "# the corresponding Date\n",
    "df_agg['outliers']=df_agg['ctr'].where(abs(df_agg['ctr']-df_agg['MA8'])>criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the outliers\n",
    "ax=df_agg.plot(y='outliers',style='o')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('CTR Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to keep all the points except the outliers ( in case we want to interpolate the missing samples ) we can\n",
    "# do it with a simple method called MASK\n",
    "no_outliers=df_agg['ctr'].mask(abs(df_agg['ctr']-df_agg['MA8'])>criteria)\n",
    "# We can plot the 'remaining' points\n",
    "ax=no_outliers.plot(style='o')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('CTR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we highligh the outliers as requested on the challenge\n",
    "ax = df_agg.plot(y='ctr')\n",
    "df_agg.plot(y='outliers',ax = ax, style='+',color='red')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('CTR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an extra we can save the timeseries/values for the outliers and the no-outliers for further processing\n",
    "df_agg['outliers'].to_csv('./data/outliers.csv', header=None,  sep=';', mode='a')\n",
    "no_outliers.to_csv('./data/no_outliers.csv', header=None,  sep=';', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
